{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tnF-IUFcJVAP",
        "outputId": "9b8e40c4-c19d-450a-974f-57b6c99e62c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "COHR_API_KEY = userdata.get('COHR_API_KEY')\n",
        "WCD_API_KEY = userdata.get('WCD_API_KEY')\n",
        "WCD_URL = userdata.get('WCD_URL')"
      ],
      "metadata": {
        "id": "5AGS0RTGJmiG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6vEs7dXi3gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8Miyf6K7mun"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "model = ChatGroq(model=\"llama3-8b-8192\", api_key=GROQ_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg = \"how to create the Advance RAG using Groq and Langchain\"\n",
        "resp = model.invoke(msg)"
      ],
      "metadata": {
        "id": "zrrwwMZHIbC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "collapsed": true,
        "id": "_uSomGK5KKCk",
        "outputId": "3f97d812-1301-4203-8bbc-c9f7492dfc78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A very specific and interesting question!\\n\\nTo create an Advanced RAG (Risk, Action, Governance) dashboard using Groq and Langchain, you'll need to follow these steps:\\n\\n**Prerequisites:**\\n\\n1. Familiarity with Groq, a query language for BigQuery.\\n2. Familiarity with Langchain, a platform for building AI-powered search and analytics applications.\\n3. A BigQuery dataset with the necessary data for your RAG dashboard.\\n\\n**Step 1: Prepare your data**\\n\\nIn your BigQuery dataset, create a table with the following columns:\\n\\n| Column | Data Type | Description |\\n| --- | --- | --- |\\n| Risk_ID | STRING | Unique identifier for each risk |\\n| Risk_Name | STRING | Name of the risk |\\n| Risk_Description | STRING | Description of the risk |\\n| Risk_Severity | STRING | Severity of the risk (e.g., Low, Medium, High) |\\n| Action_ID | STRING | Unique identifier for each action |\\n| Action_Name | STRING | Name of the action |\\n| Action_Description | STRING | Description of the action |\\n| Action_Status | STRING | Status of the action (e.g., Open, In Progress, Closed) |\\n| Governance_ID | STRING | Unique identifier for each governance element |\\n| Governance_Name | STRING | Name of the governance element |\\n| Governance_Description | STRING | Description of the governance element |\\n\\n**Step 2: Create a Groq query**\\n\\nCreate a Groq query that retrieves the necessary data for your RAG dashboard. For example:\\n```groq\\nquery RAG_Dashboard {\\n  risks: (\\n    select distinct risk_id, risk_name, risk_description, risk_severity\\n    from your_dataset.your_table\\n    where risk_severity != 'Low'\\n  ),\\n  actions: (\\n    select distinct action_id, action_name, action_description, action_status\\n    from your_dataset.your_table\\n    where action_status != 'Closed'\\n  ),\\n  governance: (\\n    select distinct governance_id, governance_name, governance_description\\n    from your_dataset.your_table\\n    where governance_type = 'Risk Governance'\\n  )\\n}\\n```\\nThis query retrieves distinct risks, actions, and governance elements from your dataset, filtering out low-severity risks and closed actions.\\n\\n**Step 3: Integrate with Langchain**\\n\\nCreate a Langchain application that consumes the Groq query results and generates the RAG dashboard. Langchain provides a GraphQL API that allows you to query and manipulate data from various sources.\\n\\nCreate a new Langchain application and add a `query` endpoint that executes the Groq query. For example:\\n```graphql\\ntype Query {\\n  RAG_Dashboard: [RAGDashboard]\\n}\\n\\ntype RAGDashboard {\\n  risks: [Risk]\\n  actions: [Action]\\n  governance: [Governance]\\n}\\n\\ntype Risk {\\n  id: String!\\n  name: String!\\n  description: String!\\n  severity: String!\\n}\\n\\ntype Action {\\n  id: String!\\n  name: String!\\n  description: String!\\n  status: String!\\n}\\n\\ntype Governance {\\n  id: String!\\n  name: String!\\n  description: String!\\n}\\n```\\nThis GraphQL schema defines the `RAG_Dashboard` query, which returns a list of `Risk`, `Action`, and `Governance` objects.\\n\\n**Step 4: Visualize the data**\\n\\nUse a frontend framework like React or Angular to create a dashboard that consumes the Langchain API and visualizes the RAG data. For example, you can use a table component to display the risks, actions, and governance elements.\\n\\nHere's a simple example using React:\\n```jsx\\nimport React from 'react';\\nimport { useQuery } from '@apollo/client';\\nimport { RAG_DASHBOARD } from './graphql/queries';\\n\\nfunction RAGDashboard() {\\n  const { data, error, loading } = useQuery(RAG_DASHBOARD);\\n\\n  if (loading) return <div>Loading...</div>;\\n  if (error) return <div>Error: {error.message}</div>;\\n\\n  return (\\n    <div>\\n      <h1>RAG Dashboard</h1>\\n      <table>\\n        <thead>\\n          <tr>\\n            <th>Risk</th>\\n            <th>Severity</th>\\n            <th>Actions</th>\\n            <th>Governance</th>\\n          </tr>\\n        </thead>\\n        <tbody>\\n          {data.risks.map((risk) => (\\n            <tr key={risk.id}>\\n              <td>{risk.name}</td>\\n              <td>{risk.severity}</td>\\n              <td>\\n                {data.actions.filter((action) => action.risk_id === risk.id).map((action) => (\\n                  <span key={action.id}>{action.name}</span>\\n                ))}\\n              </td>\\n              <td>\\n                {data.governance.filter((governance) => governance.risk_id === risk.id).map((governance) => (\\n                  <span key={governance.id}>{governance.name}</span>\\n                ))}\\n              </td>\\n            </tr>\\n          ))}\\n        </tbody>\\n      </table>\\n    </div>\\n  );\\n}\\n\\nexport default RAGDashboard;\\n```\\nThis React component uses the `useQuery` hook to fetch the `RAG_Dashboard` data from the Langchain API. It then renders a table with the risk data, including the severity, actions, and governance elements.\\n\\nThat's it! You now have an Advanced RAG dashboard built using Groq and Langchain.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-core"
      ],
      "metadata": {
        "id": "VtMOOzg7Mxz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "help(InMemoryVectorStore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3aphUb7ShtaU",
        "outputId": "fbfe8bab-a34d-4491-a564-3969d673b737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class InMemoryVectorStore in module langchain_core.vectorstores.in_memory:\n",
            "\n",
            "class InMemoryVectorStore(langchain_core.vectorstores.base.VectorStore)\n",
            " |  InMemoryVectorStore(embedding: 'Embeddings') -> 'None'\n",
            " |  \n",
            " |  In-memory vector store implementation.\n",
            " |  \n",
            " |  Uses a dictionary, and computes cosine similarity for search using numpy.\n",
            " |  \n",
            " |  Setup:\n",
            " |      Install ``langchain-core``.\n",
            " |  \n",
            " |      .. code-block:: bash\n",
            " |  \n",
            " |          pip install -U langchain-core\n",
            " |  \n",
            " |  Key init args — indexing params:\n",
            " |      embedding_function: Embeddings\n",
            " |          Embedding function to use.\n",
            " |  \n",
            " |  Instantiate:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          from langchain_core.vectorstores import InMemoryVectorStore\n",
            " |          from langchain_openai import OpenAIEmbeddings\n",
            " |  \n",
            " |          vector_store = InMemoryVectorStore(OpenAIEmbeddings())\n",
            " |  \n",
            " |  Add Documents:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          from langchain_core.documents import Document\n",
            " |  \n",
            " |          document_1 = Document(id=\"1\", page_content=\"foo\", metadata={\"baz\": \"bar\"})\n",
            " |          document_2 = Document(id=\"2\", page_content=\"thud\", metadata={\"bar\": \"baz\"})\n",
            " |          document_3 = Document(id=\"3\", page_content=\"i will be deleted :(\")\n",
            " |  \n",
            " |          documents = [document_1, document_2, document_3]\n",
            " |          vector_store.add_documents(documents=documents)\n",
            " |  \n",
            " |  Delete Documents:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          vector_store.delete(ids=[\"3\"])\n",
            " |  \n",
            " |  Search:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          results = vector_store.similarity_search(query=\"thud\",k=1)\n",
            " |          for doc in results:\n",
            " |              print(f\"* {doc.page_content} [{doc.metadata}]\")\n",
            " |  \n",
            " |      .. code-block:: none\n",
            " |  \n",
            " |          * thud [{'bar': 'baz'}]\n",
            " |  \n",
            " |  Search with filter:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          def _filter_function(doc: Document) -> bool:\n",
            " |              return doc.metadata.get(\"bar\") == \"baz\"\n",
            " |  \n",
            " |          results = vector_store.similarity_search(\n",
            " |              query=\"thud\", k=1, filter=_filter_function\n",
            " |          )\n",
            " |          for doc in results:\n",
            " |              print(f\"* {doc.page_content} [{doc.metadata}]\")\n",
            " |  \n",
            " |      .. code-block:: none\n",
            " |  \n",
            " |          * thud [{'bar': 'baz'}]\n",
            " |  \n",
            " |  \n",
            " |  Search with score:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          results = vector_store.similarity_search_with_score(\n",
            " |              query=\"qux\", k=1\n",
            " |          )\n",
            " |          for doc, score in results:\n",
            " |              print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")\n",
            " |  \n",
            " |      .. code-block:: none\n",
            " |  \n",
            " |          * [SIM=0.832268] foo [{'baz': 'bar'}]\n",
            " |  \n",
            " |  Async:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          # add documents\n",
            " |          # await vector_store.aadd_documents(documents=documents)\n",
            " |  \n",
            " |          # delete documents\n",
            " |          # await vector_store.adelete(ids=[\"3\"])\n",
            " |  \n",
            " |          # search\n",
            " |          # results = vector_store.asimilarity_search(query=\"thud\", k=1)\n",
            " |  \n",
            " |          # search with score\n",
            " |          results = await vector_store.asimilarity_search_with_score(query=\"qux\", k=1)\n",
            " |          for doc,score in results:\n",
            " |              print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")\n",
            " |  \n",
            " |      .. code-block:: none\n",
            " |  \n",
            " |          * [SIM=0.832268] foo [{'baz': 'bar'}]\n",
            " |  \n",
            " |  Use as Retriever:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          retriever = vector_store.as_retriever(\n",
            " |              search_type=\"mmr\",\n",
            " |              search_kwargs={\"k\": 1, \"fetch_k\": 2, \"lambda_mult\": 0.5},\n",
            " |          )\n",
            " |          retriever.invoke(\"thud\")\n",
            " |  \n",
            " |      .. code-block:: none\n",
            " |  \n",
            " |          [Document(id='2', metadata={'bar': 'baz'}, page_content='thud')]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      InMemoryVectorStore\n",
            " |      langchain_core.vectorstores.base.VectorStore\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, embedding: 'Embeddings') -> 'None'\n",
            " |      Initialize with the given embedding function.\n",
            " |      \n",
            " |      Args:\n",
            " |          embedding: embedding function to use.\n",
            " |  \n",
            " |  async aadd_documents(self, documents: 'list[Document]', ids: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'list[str]'\n",
            " |      Add documents to the store.\n",
            " |  \n",
            " |  add_documents(self, documents: 'list[Document]', ids: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'list[str]'\n",
            " |      Add documents to the store.\n",
            " |  \n",
            " |  async adelete(self, ids: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'None'\n",
            " |      Async delete by vector ID or other criteria.\n",
            " |      \n",
            " |      Args:\n",
            " |          ids: List of ids to delete. If None, delete all. Default is None.\n",
            " |          **kwargs: Other keyword arguments that subclasses might use.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Optional[bool]: True if deletion is successful,\n",
            " |          False otherwise, None if not implemented.\n",
            " |  \n",
            " |  async aget_by_ids(self, ids: 'Sequence[str]', /) -> 'list[Document]'\n",
            " |      Async get documents by their ids.\n",
            " |      \n",
            " |      Args:\n",
            " |          ids: The ids of the documents to get.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of Document objects.\n",
            " |  \n",
            " |  async amax_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Async return docs selected using the maximal marginal relevance.\n",
            " |      \n",
            " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
            " |      among selected documents.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Text to look up documents similar to.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
            " |              Default is 20.\n",
            " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
            " |              of diversity among the results with 0 corresponding\n",
            " |              to maximum diversity and 1 to minimum diversity.\n",
            " |              Defaults to 0.5.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents selected by maximal marginal relevance.\n",
            " |  \n",
            " |  async asimilarity_search(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Async return docs most similar to query.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Input text.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents most similar to the query.\n",
            " |  \n",
            " |  async asimilarity_search_by_vector(self, embedding: 'list[float]', k: 'int' = 4, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Async return docs most similar to embedding vector.\n",
            " |      \n",
            " |      Args:\n",
            " |          embedding: Embedding to look up documents similar to.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents most similar to the query vector.\n",
            " |  \n",
            " |  async asimilarity_search_with_score(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'list[tuple[Document, float]]'\n",
            " |      Async run similarity search with distance.\n",
            " |      \n",
            " |      Args:\n",
            " |          *args: Arguments to pass to the search method.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Tuples of (doc, similarity_score).\n",
            " |  \n",
            " |  async aupsert(self, items: 'Sequence[Document]', /, **kwargs: 'Any') -> 'UpsertResponse'\n",
            " |      .. deprecated:: 0.2.29 This was a beta API that was added in 0.2.11. It'll be removed in 0.3.0. Use :meth:`~VectorStore.aadd_documents` instead. It will not be removed until langchain-core==1.0.\n",
            " |  \n",
            " |  delete(self, ids: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'None'\n",
            " |      Delete by vector ID or other criteria.\n",
            " |      \n",
            " |      Args:\n",
            " |          ids: List of ids to delete. If None, delete all. Default is None.\n",
            " |          **kwargs: Other keyword arguments that subclasses might use.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Optional[bool]: True if deletion is successful,\n",
            " |          False otherwise, None if not implemented.\n",
            " |  \n",
            " |  dump(self, path: 'str') -> 'None'\n",
            " |      Dump the vector store to a file.\n",
            " |      \n",
            " |      Args:\n",
            " |          path: The path to dump the vector store to.\n",
            " |  \n",
            " |  get_by_ids(self, ids: 'Sequence[str]', /) -> 'list[Document]'\n",
            " |      Get documents by their ids.\n",
            " |      \n",
            " |      Args:\n",
            " |          ids: The ids of the documents to get.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of Document objects.\n",
            " |  \n",
            " |  max_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Return docs selected using the maximal marginal relevance.\n",
            " |      \n",
            " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
            " |      among selected documents.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Text to look up documents similar to.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
            " |              Default is 20.\n",
            " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
            " |              of diversity among the results with 0 corresponding\n",
            " |              to maximum diversity and 1 to minimum diversity.\n",
            " |              Defaults to 0.5.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents selected by maximal marginal relevance.\n",
            " |  \n",
            " |  max_marginal_relevance_search_by_vector(self, embedding: 'list[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Return docs selected using the maximal marginal relevance.\n",
            " |      \n",
            " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
            " |      among selected documents.\n",
            " |      \n",
            " |      Args:\n",
            " |          embedding: Embedding to look up documents similar to.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
            " |              Default is 20.\n",
            " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
            " |              of diversity among the results with 0 corresponding\n",
            " |              to maximum diversity and 1 to minimum diversity.\n",
            " |              Defaults to 0.5.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents selected by maximal marginal relevance.\n",
            " |  \n",
            " |  similarity_search(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Return docs most similar to query.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Input text.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents most similar to the query.\n",
            " |  \n",
            " |  similarity_search_by_vector(self, embedding: 'list[float]', k: 'int' = 4, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Return docs most similar to embedding vector.\n",
            " |      \n",
            " |      Args:\n",
            " |          embedding: Embedding to look up documents similar to.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents most similar to the query vector.\n",
            " |  \n",
            " |  similarity_search_with_score(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'list[tuple[Document, float]]'\n",
            " |      Run similarity search with distance.\n",
            " |      \n",
            " |      Args:\n",
            " |          *args: Arguments to pass to the search method.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Tuples of (doc, similarity_score).\n",
            " |  \n",
            " |  similarity_search_with_score_by_vector(self, embedding: 'list[float]', k: 'int' = 4, filter: 'Optional[Callable[[Document], bool]]' = None, **kwargs: 'Any') -> 'list[tuple[Document, float]]'\n",
            " |  \n",
            " |  upsert(self, items: 'Sequence[Document]', /, **kwargs: 'Any') -> 'UpsertResponse'\n",
            " |      .. deprecated:: 0.2.29 This was a beta API that was added in 0.2.11. It'll be removed in 0.3.0. Use :meth:`~VectorStore.add_documents` instead. It will not be removed until langchain-core==1.0.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  async afrom_texts(texts: 'list[str]', embedding: 'Embeddings', metadatas: 'Optional[list[dict]]' = None, **kwargs: 'Any') -> 'InMemoryVectorStore' from abc.ABCMeta\n",
            " |      Async return VectorStore initialized from texts and embeddings.\n",
            " |      \n",
            " |      Args:\n",
            " |          texts: Texts to add to the vectorstore.\n",
            " |          embedding: Embedding function to use.\n",
            " |          metadatas: Optional list of metadatas associated with the texts.\n",
            " |              Default is None.\n",
            " |          ids: Optional list of IDs associated with the texts.\n",
            " |          kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          VectorStore: VectorStore initialized from texts and embeddings.\n",
            " |  \n",
            " |  from_texts(texts: 'list[str]', embedding: 'Embeddings', metadatas: 'Optional[list[dict]]' = None, **kwargs: 'Any') -> 'InMemoryVectorStore' from abc.ABCMeta\n",
            " |      Return VectorStore initialized from texts and embeddings.\n",
            " |      \n",
            " |      Args:\n",
            " |          texts: Texts to add to the vectorstore.\n",
            " |          embedding: Embedding function to use.\n",
            " |          metadatas: Optional list of metadatas associated with the texts.\n",
            " |              Default is None.\n",
            " |          ids: Optional list of IDs associated with the texts.\n",
            " |          kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          VectorStore: VectorStore initialized from texts and embeddings.\n",
            " |  \n",
            " |  load(path: 'str', embedding: 'Embeddings', **kwargs: 'Any') -> 'InMemoryVectorStore' from abc.ABCMeta\n",
            " |      Load a vector store from a file.\n",
            " |      \n",
            " |      Args:\n",
            " |          path: The path to load the vector store from.\n",
            " |          embedding: The embedding to use.\n",
            " |          kwargs: Additional arguments to pass to the constructor.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A VectorStore object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  embeddings\n",
            " |      Access the query embedding object if available.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.vectorstores.base.VectorStore:\n",
            " |  \n",
            " |  async aadd_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[list[dict]]' = None, *, ids: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'list[str]'\n",
            " |      Async run more texts through the embeddings and add to the vectorstore.\n",
            " |      \n",
            " |      Args:\n",
            " |          texts: Iterable of strings to add to the vectorstore.\n",
            " |          metadatas: Optional list of metadatas associated with the texts.\n",
            " |              Default is None.\n",
            " |          ids: Optional list\n",
            " |          **kwargs: vectorstore specific parameters.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of ids from adding the texts into the vectorstore.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the number of metadatas does not match the number of texts.\n",
            " |          ValueError: If the number of ids does not match the number of texts.\n",
            " |  \n",
            " |  add_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[list[dict]]' = None, *, ids: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'list[str]'\n",
            " |      Run more texts through the embeddings and add to the vectorstore.\n",
            " |      \n",
            " |      Args:\n",
            " |          texts: Iterable of strings to add to the vectorstore.\n",
            " |          metadatas: Optional list of metadatas associated with the texts.\n",
            " |          ids: Optional list of IDs associated with the texts.\n",
            " |          **kwargs: vectorstore specific parameters.\n",
            " |              One of the kwargs should be `ids` which is a list of ids\n",
            " |              associated with the texts.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of ids from adding the texts into the vectorstore.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the number of metadatas does not match the number of texts.\n",
            " |          ValueError: If the number of ids does not match the number of texts.\n",
            " |  \n",
            " |  async amax_marginal_relevance_search_by_vector(self, embedding: 'list[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Async return docs selected using the maximal marginal relevance.\n",
            " |      \n",
            " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
            " |      among selected documents.\n",
            " |      \n",
            " |      Args:\n",
            " |          embedding: Embedding to look up documents similar to.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
            " |              Default is 20.\n",
            " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
            " |              of diversity among the results with 0 corresponding\n",
            " |              to maximum diversity and 1 to minimum diversity.\n",
            " |              Defaults to 0.5.\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents selected by maximal marginal relevance.\n",
            " |  \n",
            " |  as_retriever(self, **kwargs: 'Any') -> 'VectorStoreRetriever'\n",
            " |      Return VectorStoreRetriever initialized from this VectorStore.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs: Keyword arguments to pass to the search function.\n",
            " |              Can include:\n",
            " |              search_type (Optional[str]): Defines the type of search that\n",
            " |                  the Retriever should perform.\n",
            " |                  Can be \"similarity\" (default), \"mmr\", or\n",
            " |                  \"similarity_score_threshold\".\n",
            " |              search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
            " |                  search function. Can include things like:\n",
            " |                      k: Amount of documents to return (Default: 4)\n",
            " |                      score_threshold: Minimum relevance threshold\n",
            " |                          for similarity_score_threshold\n",
            " |                      fetch_k: Amount of documents to pass to MMR algorithm\n",
            " |                          (Default: 20)\n",
            " |                      lambda_mult: Diversity of results returned by MMR;\n",
            " |                          1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
            " |                      filter: Filter by document metadata\n",
            " |      \n",
            " |      Returns:\n",
            " |          VectorStoreRetriever: Retriever class for VectorStore.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          # Retrieve more documents with higher diversity\n",
            " |          # Useful if your dataset has many similar documents\n",
            " |          docsearch.as_retriever(\n",
            " |              search_type=\"mmr\",\n",
            " |              search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
            " |          )\n",
            " |      \n",
            " |          # Fetch more documents for the MMR algorithm to consider\n",
            " |          # But only return the top 5\n",
            " |          docsearch.as_retriever(\n",
            " |              search_type=\"mmr\",\n",
            " |              search_kwargs={'k': 5, 'fetch_k': 50}\n",
            " |          )\n",
            " |      \n",
            " |          # Only retrieve documents that have a relevance score\n",
            " |          # Above a certain threshold\n",
            " |          docsearch.as_retriever(\n",
            " |              search_type=\"similarity_score_threshold\",\n",
            " |              search_kwargs={'score_threshold': 0.8}\n",
            " |          )\n",
            " |      \n",
            " |          # Only get the single most similar document from the dataset\n",
            " |          docsearch.as_retriever(search_kwargs={'k': 1})\n",
            " |      \n",
            " |          # Use a filter to only retrieve documents from a specific paper\n",
            " |          docsearch.as_retriever(\n",
            " |              search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
            " |          )\n",
            " |  \n",
            " |  async asearch(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Async return docs most similar to query using a specified search type.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Input text.\n",
            " |          search_type: Type of search to perform. Can be \"similarity\",\n",
            " |              \"mmr\", or \"similarity_score_threshold\".\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents most similar to the query.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If search_type is not one of \"similarity\",\n",
            " |              \"mmr\", or \"similarity_score_threshold\".\n",
            " |  \n",
            " |  async asimilarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'list[tuple[Document, float]]'\n",
            " |      Async return docs and relevance scores in the range [0, 1].\n",
            " |      \n",
            " |      0 is dissimilar, 1 is most similar.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Input text.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
            " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
            " |                  filter the resulting set of retrieved docs\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Tuples of (doc, similarity_score)\n",
            " |  \n",
            " |  search(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'list[Document]'\n",
            " |      Return docs most similar to query using a specified search type.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Input text\n",
            " |          search_type: Type of search to perform. Can be \"similarity\",\n",
            " |              \"mmr\", or \"similarity_score_threshold\".\n",
            " |          **kwargs: Arguments to pass to the search method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Documents most similar to the query.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If search_type is not one of \"similarity\",\n",
            " |              \"mmr\", or \"similarity_score_threshold\".\n",
            " |  \n",
            " |  similarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'list[tuple[Document, float]]'\n",
            " |      Return docs and relevance scores in the range [0, 1].\n",
            " |      \n",
            " |      0 is dissimilar, 1 is most similar.\n",
            " |      \n",
            " |      Args:\n",
            " |          query: Input text.\n",
            " |          k: Number of Documents to return. Defaults to 4.\n",
            " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
            " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
            " |                  filter the resulting set of retrieved docs.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of Tuples of (doc, similarity_score).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.vectorstores.base.VectorStore:\n",
            " |  \n",
            " |  async afrom_documents(documents: 'list[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
            " |      Async return VectorStore initialized from documents and embeddings.\n",
            " |      \n",
            " |      Args:\n",
            " |          documents: List of Documents to add to the vectorstore.\n",
            " |          embedding: Embedding function to use.\n",
            " |          kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          VectorStore: VectorStore initialized from documents and embeddings.\n",
            " |  \n",
            " |  from_documents(documents: 'list[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
            " |      Return VectorStore initialized from documents and embeddings.\n",
            " |      \n",
            " |      Args:\n",
            " |          documents: List of Documents to add to the vectorstore.\n",
            " |          embedding: Embedding function to use.\n",
            " |          kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          VectorStore: VectorStore initialized from documents and embeddings.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from langchain_core.vectorstores.base.VectorStore:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nCBNSnvljSda",
        "outputId": "4905a033-6cd0-429f-b971-131edca26a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/250.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.2/250.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import CohereEmbeddings"
      ],
      "metadata": {
        "id": "26EMRoFdhzT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_type: This parameter tells Cohere the type of text you are embedding. Possible values are:\n",
        "# \"search_document\" (default): Use for embeddings stored in a vector database for search.\n",
        "# \"search_query\": Use for embeddings of search queries against a vector DB.\n",
        "# \"classification\": Use for embeddings passed through a text classifier.\n",
        "# \"clustering\": Use for embeddings run through a clustering algorithm\n",
        "\n",
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\",\n",
        "                                cohere_api_key = COHR_API_KEY\n",
        "                              )"
      ],
      "metadata": {
        "id": "7NIRFUg4mreH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(CohereEmbeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R9FxuQNyjOmx",
        "outputId": "e3170a19-f012-46d9-f470-cc3597e299db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class CohereEmbeddings in module langchain_cohere.embeddings:\n",
            "\n",
            "class CohereEmbeddings(pydantic.main.BaseModel, langchain_core.embeddings.embeddings.Embeddings)\n",
            " |  CohereEmbeddings(*, client: Any, async_client: Any, model: Optional[str] = None, truncate: Optional[str] = None, cohere_api_key: Optional[pydantic.types.SecretStr] = <factory>, embedding_types: Sequence[str] = ['float'], max_retries: int = 3, request_timeout: Optional[float] = None, user_agent: str = 'langchain:partner', base_url: Optional[str] = None) -> None\n",
            " |  \n",
            " |  Implements the Embeddings interface with Cohere's text representation language\n",
            " |  models.\n",
            " |  \n",
            " |  Find out more about us at https://cohere.com and https://huggingface.co/CohereForAI\n",
            " |  \n",
            " |  This implementation uses the Embed API - see https://docs.cohere.com/reference/embed\n",
            " |  \n",
            " |  To use this you'll need to a Cohere API key - either pass it to cohere_api_key\n",
            " |  parameter or set the COHERE_API_KEY environment variable.\n",
            " |  \n",
            " |  API keys are available on https://cohere.com - it's free to sign up and trial API\n",
            " |  keys work with this implementation.\n",
            " |  \n",
            " |  Basic Example:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          cohere_embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
            " |          text = \"This is a test document.\"\n",
            " |  \n",
            " |          query_result = cohere_embeddings.embed_query(text)\n",
            " |          print(query_result)\n",
            " |  \n",
            " |          doc_result = cohere_embeddings.embed_documents([text])\n",
            " |          print(doc_result)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CohereEmbeddings\n",
            " |      pydantic.main.BaseModel\n",
            " |      langchain_core.embeddings.embeddings.Embeddings\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  async aembed(self, texts: List[str], *, input_type: Union[Literal['search_document', 'search_query', 'classification', 'clustering', 'image'], Any, NoneType] = None) -> List[List[float]]\n",
            " |  \n",
            " |  async aembed_documents(self, texts: List[str]) -> List[List[float]]\n",
            " |      Async call out to Cohere's embedding endpoint.\n",
            " |      \n",
            " |      Args:\n",
            " |          texts: The list of texts to embed.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of embeddings, one for each text.\n",
            " |  \n",
            " |  async aembed_query(self, text: str) -> List[float]\n",
            " |      Async call out to Cohere's embedding endpoint.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: The text to embed.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Embeddings for the text.\n",
            " |  \n",
            " |  aembed_with_retry(self, **kwargs: Any) -> Any\n",
            " |      Use tenacity to retry the embed call.\n",
            " |  \n",
            " |  embed(self, texts: List[str], *, input_type: Union[Literal['search_document', 'search_query', 'classification', 'clustering', 'image'], Any, NoneType] = None) -> List[List[float]]\n",
            " |  \n",
            " |  embed_documents(self, texts: List[str]) -> List[List[float]]\n",
            " |      Embed a list of document texts.\n",
            " |      \n",
            " |      Args:\n",
            " |          texts: The list of texts to embed.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List of embeddings, one for each text.\n",
            " |  \n",
            " |  embed_query(self, text: str) -> List[float]\n",
            " |      Call out to Cohere's embedding endpoint.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: The text to embed.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Embeddings for the text.\n",
            " |  \n",
            " |  embed_with_retry(self, **kwargs: Any) -> Any\n",
            " |      Use tenacity to retry the embed call.\n",
            " |  \n",
            " |  validate_model_specified(self) -> typing_extensions.Self\n",
            " |      Validate that model is specified.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  validate_environment(values: Dict) -> Any from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Validate that api key and python package exists in environment.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'async_client': typing.Any, 'base_url': typing.Opti...\n",
            " |  \n",
            " |  __class_vars__ = set()\n",
            " |  \n",
            " |  __private_attributes__ = {}\n",
            " |  \n",
            " |  __pydantic_complete__ = True\n",
            " |  \n",
            " |  __pydantic_computed_fields__ = {}\n",
            " |  \n",
            " |  __pydantic_core_schema__ = {'function': {'function': <function CohereE...\n",
            " |  \n",
            " |  __pydantic_custom_init__ = False\n",
            " |  \n",
            " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
            " |  \n",
            " |  __pydantic_fields__ = {'async_client': FieldInfo(annotation=Any, requi...\n",
            " |  \n",
            " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
            " |  \n",
            " |  __pydantic_parent_namespace__ = None\n",
            " |  \n",
            " |  __pydantic_post_init__ = None\n",
            " |  \n",
            " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
            " |      Model...\n",
            " |  \n",
            " |  __pydantic_validator__ = SchemaValidator(title=\"CohereEmbeddings\", val...\n",
            " |  \n",
            " |  __signature__ = <Signature (*, client: Any, async_client: Any, m...art...\n",
            " |  \n",
            " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'forbid', 'p...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __copy__(self) -> 'Self'\n",
            " |      Returns a shallow copy of the model.\n",
            " |  \n",
            " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
            " |      Returns a deep copy of the model.\n",
            " |  \n",
            " |  __delattr__(self, item: 'str') -> 'Any'\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __eq__(self, other: 'Any') -> 'bool'\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __getattr__(self, item: 'str') -> 'Any'\n",
            " |  \n",
            " |  __getstate__(self) -> 'dict[Any, Any]'\n",
            " |  \n",
            " |  __init__(self, /, **data: 'Any') -> 'None'\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |      \n",
            " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
            " |      validated to form a valid model.\n",
            " |      \n",
            " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
            " |  \n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      So `dict(model)` works.\n",
            " |  \n",
            " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
            " |  \n",
            " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
            " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
            " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
            " |  \n",
            " |  __repr__(self) -> 'str'\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
            " |  \n",
            " |  __repr_name__(self) -> 'str'\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |  \n",
            " |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
            " |      Returns the string representation of a recursive object.\n",
            " |  \n",
            " |  __repr_str__(self, join_str: 'str') -> 'str'\n",
            " |  \n",
            " |  __rich_repr__(self) -> 'RichReprResult'\n",
            " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
            " |  \n",
            " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
            " |  \n",
            " |  __str__(self) -> 'str'\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      Returns a copy of the model.\n",
            " |      \n",
            " |      !!! warning \"Deprecated\"\n",
            " |          This method is now deprecated; use `model_copy` instead.\n",
            " |      \n",
            " |      If you need `include` or `exclude`, use:\n",
            " |      \n",
            " |      ```python {test=\"skip\" lint=\"skip\"}\n",
            " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
            " |      data = {**data, **(update or {})}\n",
            " |      copied = self.model_validate(data)\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
            " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
            " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
            " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A copy of the model with included, excluded and updated fields as specified.\n",
            " |  \n",
            " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
            " |  \n",
            " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
            " |  \n",
            " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
            " |      \n",
            " |      Returns a copy of the model.\n",
            " |      \n",
            " |      Args:\n",
            " |          update: Values to change/add in the new model. Note: the data is not validated\n",
            " |              before creating the new model. You should trust this data.\n",
            " |          deep: Set to `True` to make a deep copy of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          New model instance.\n",
            " |  \n",
            " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
            " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
            " |      \n",
            " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
            " |      \n",
            " |      Args:\n",
            " |          mode: The mode in which `to_python` should run.\n",
            " |              If mode is 'json', the output will only contain JSON serializable types.\n",
            " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
            " |          include: A set of fields to include in the output.\n",
            " |          exclude: A set of fields to exclude from the output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A dictionary representation of the model.\n",
            " |  \n",
            " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
            " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
            " |      \n",
            " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
            " |      \n",
            " |      Args:\n",
            " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
            " |          include: Field(s) to include in the JSON output.\n",
            " |          exclude: Field(s) to exclude from the JSON output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to serialize using field aliases.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string representation of the model.\n",
            " |  \n",
            " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
            " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
            " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Hook into generating the model's CoreSchema.\n",
            " |      \n",
            " |      Args:\n",
            " |          source: The class we are generating a schema for.\n",
            " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
            " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `pydantic-core` `CoreSchema`.\n",
            " |  \n",
            " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Hook into generating the model's JSON schema.\n",
            " |      \n",
            " |      Args:\n",
            " |          core_schema: A `pydantic-core` CoreSchema.\n",
            " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
            " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
            " |              or just call the handler with the original schema.\n",
            " |          handler: Call into Pydantic's internal JSON schema generation.\n",
            " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
            " |              generation fails.\n",
            " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
            " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
            " |              for a type.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON schema, as a Python object.\n",
            " |  \n",
            " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
            " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
            " |      be present when this is called.\n",
            " |      \n",
            " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
            " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
            " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
            " |      \n",
            " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
            " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
            " |              by pydantic.\n",
            " |  \n",
            " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Creates a new instance of the `Model` class with validated data.\n",
            " |      \n",
            " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |      \n",
            " |      !!! note\n",
            " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
            " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
            " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
            " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
            " |          an error if extra values are passed, but they will be ignored.\n",
            " |      \n",
            " |      Args:\n",
            " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
            " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
            " |              Otherwise, the field names from the `values` argument will be used.\n",
            " |          values: Trusted or pre-validated data dictionary.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A new instance of the `Model` class with validated data.\n",
            " |  \n",
            " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Generates a JSON schema for a model class.\n",
            " |      \n",
            " |      Args:\n",
            " |          by_alias: Whether to use attribute aliases or not.\n",
            " |          ref_template: The reference template.\n",
            " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
            " |              `GenerateJsonSchema` with your desired modifications\n",
            " |          mode: The mode in which to generate the schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The JSON schema for the given model class.\n",
            " |  \n",
            " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Compute the class name for parametrizations of generic classes.\n",
            " |      \n",
            " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
            " |      \n",
            " |      Args:\n",
            " |          params: Tuple of types of the class. Given a generic class\n",
            " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
            " |              the value `(str, int)` would be passed to `params`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
            " |      \n",
            " |      Raises:\n",
            " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
            " |  \n",
            " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Try to rebuild the pydantic-core schema for the model.\n",
            " |      \n",
            " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
            " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
            " |      \n",
            " |      Args:\n",
            " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
            " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
            " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
            " |          _types_namespace: The types namespace, defaults to `None`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
            " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
            " |  \n",
            " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Validate a pydantic model instance.\n",
            " |      \n",
            " |      Args:\n",
            " |          obj: The object to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          from_attributes: Whether to extract data from object attributes.\n",
            " |          context: Additional context to pass to the validator.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValidationError: If the object could not be validated.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The validated model instance.\n",
            " |  \n",
            " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
            " |      \n",
            " |      Validate the given JSON data against the Pydantic model.\n",
            " |      \n",
            " |      Args:\n",
            " |          json_data: The JSON data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
            " |  \n",
            " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |      Validate the given object with string data against the Pydantic model.\n",
            " |      \n",
            " |      Args:\n",
            " |          obj: The object containing string data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |  \n",
            " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __fields_set__\n",
            " |  \n",
            " |  model_computed_fields\n",
            " |      Get metadata about the computed fields defined on the model.\n",
            " |      \n",
            " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
            " |      In V3, this property will be removed from the `BaseModel` class.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
            " |  \n",
            " |  model_extra\n",
            " |      Get extra fields set during validation.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
            " |  \n",
            " |  model_fields\n",
            " |      Get metadata about the fields defined on the model.\n",
            " |      \n",
            " |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
            " |      In V3, this property will be removed from the `BaseModel` class.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
            " |  \n",
            " |  model_fields_set\n",
            " |      Returns the set of fields that have been explicitly set on this model instance.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A set of strings representing the fields that have been set,\n",
            " |              i.e. that were not filled from defaults.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __pydantic_extra__\n",
            " |  \n",
            " |  __pydantic_fields_set__\n",
            " |  \n",
            " |  __pydantic_private__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __pydantic_root_model__ = False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-weaviate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BkgUG_g2QXqB",
        "outputId": "ff685c54-7a1c-41a8-d0e6-c51b566901b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.8/223.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
        "from weaviate.classes.init import Auth\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WCD_URL,  # Replace with your Weaviate Cloud URL\n",
        "    auth_credentials=Auth.api_key(WCD_API_KEY),  # Replace with your Weaviate Cloud key\n",
        "    )\n",
        "# Check connection\n",
        "client.is_ready()"
      ],
      "metadata": {
        "id": "Q2JIJLpZjbNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1139b1a3-d930-41ca-a208-94308982d105"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs =  [\n",
        "    Document(\n",
        "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
        "        metadata={\"source\": \"mammal-pets-doc\"},\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "H1it8UlGQT9U"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZofC1pZJjvwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = WeaviateVectorStore.from_documents(docs, embeddings, client=client)\n",
        "\n"
      ],
      "metadata": {
        "id": "v3rQGB1bjvt4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa_s_morkCeX",
        "outputId": "6a7948ab-a238-4982-920e-44c0dc09c42f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_weaviate.vectorstores.WeaviateVectorStore at 0x79a3a7bd7fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "# Print the first 100 characters of each result\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:100] + \"...\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRjE0alNkay_",
        "outputId": "745917d9-2b52-4aeb-b9f3-d38e903cce64"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1:\n",
            "Dogs are great companions, known for their loyalty and friendliness....\n",
            "\n",
            "Document 2:\n",
            "Cats are independent pets that often enjoy their own space....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GnsRz2JIkgnb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}